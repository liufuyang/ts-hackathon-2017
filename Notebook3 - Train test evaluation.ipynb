{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './sub_data_2/'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pickle.load(open(DATA_FOLDER + 'featureset.pickle', 'rb'))\n",
    "Y = pickle.load(open(DATA_FOLDER + 'labels_int.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np, X_dev_test_np, y_train_np, y_dev_test_np = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_dev_np, X_test_np, y_dev_np, y_test_np = train_test_split(X_dev_test_np, y_dev_test_np, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "normalizer = preprocessing.Normalizer().fit(X_train_np)\n",
    "\n",
    "#X_train_np = normalizer.transform(X_train_np)\n",
    "#X_dev_np = normalizer.transform(X_dev_np)\n",
    "#X_test_np = normalizer.transform(X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47773 10237 10238\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_np), len(X_dev_np), len(X_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47773, 2920)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2920 2699\n"
     ]
    }
   ],
   "source": [
    "_, feature_length = X_train_np.shape\n",
    "class_length =  len(np.unique(Y))\n",
    "\n",
    "print(feature_length, class_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train_np).float()\n",
    "y_train = torch.LongTensor(y_train_np)\n",
    "\n",
    "X_dev = torch.from_numpy(X_dev_np).float()\n",
    "y_dev = torch.LongTensor(y_dev_np)\n",
    "\n",
    "x, y = Variable(X_train), Variable(y_train)\n",
    "x_dev, y_dev = Variable(X_dev), y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential (\n",
      "  (0): Linear (2920 -> 500)\n",
      "  (1): ReLU ()\n",
      "  (2): Linear (500 -> 2699)\n",
      "  (3): LogSoftmax ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(feature_length, 500),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(500, class_length),\n",
    "    torch.nn.LogSoftmax()\n",
    ")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.02)\n",
    "loss_func = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 -> Loss=7.906998, Train Acc=0.000, Dev Acc=0.273\n",
      "Epoch  2 -> Loss=5.534091, Train Acc=0.280, Dev Acc=0.501\n",
      "Epoch  3 -> Loss=3.791727, Train Acc=0.510, Dev Acc=0.574\n",
      "Epoch  7 -> Loss=1.115723, Train Acc=0.829, Dev Acc=0.846\n",
      "Epoch 12 -> Loss=0.312775, Train Acc=0.944, Dev Acc=0.916\n",
      "Epoch 17 -> Loss=0.122690, Train Acc=0.975, Dev Acc=0.935\n",
      "Epoch 22 -> Loss=0.062960, Train Acc=0.986, Dev Acc=0.942\n",
      "Epoch 27 -> Loss=0.038710, Train Acc=0.991, Dev Acc=0.948\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4c0209c9db68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# clear gradients for next train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tradeshift/ts-hackathon-2017/p3ml-venv/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tradeshift/ts-hackathon-2017/p3ml-venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(100):\n",
    "    out = net(x)                 # input x and predict based on x\n",
    "    loss = loss_func(out, y)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    if t % 5 == 1 or t in [0, 2]:\n",
    "        # plot and show learning process\n",
    "        _, prediction = torch.max(out, 1)\n",
    "        pred_y = prediction.data.numpy().squeeze()\n",
    "        target_y = y.data.numpy()\n",
    "        \n",
    "        \n",
    "        out_dev = net(x_dev)\n",
    "        _, prediction_dev = torch.max(out_dev, 1)\n",
    "        pred_y_dev = prediction_dev.data.numpy().squeeze()\n",
    "        target_y_dev = y_dev.numpy()\n",
    "\n",
    "        acc_train = sum(pred_y == target_y)/len(target_y)\n",
    "        acc_dev = sum(pred_y_dev == target_y_dev)/len(target_y_dev)\n",
    "        \n",
    "        print('Epoch{0:3d} -> Loss={1:.6f}, Train Acc={2:.3f}, Dev Acc={3:.3f}'.format(t+1, loss.data[0], acc_train, acc_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65835231, -0.40824829,  0.62347969],\n",
       "       [ 0.64544344,  0.81649658,  0.77934961],\n",
       "       [ 0.38726606,  0.40824829,  0.06234797]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_train = np.array([[ 51, -1.,  2000.],\n",
    "                    [ 50.,  2.,  2500.],\n",
    "                   [ 30.,  1., 200.]])\n",
    "\n",
    "scaler = preprocessing.Normalizer().fit(X_train.T)\n",
    "scaler.transform(X_train.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2699"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2699"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(np.array(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47773])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with some example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[394, 2685, 2579, 1584, 779]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_np[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sender_dict = pickle.load(open(DATA_FOLDER + 'doc_sender_dict.pickle', 'rb'))\n",
    "sender_id_all = pickle.load(open(DATA_FOLDER +'sender_id_all.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0727e23c-ae0f-45e8-a110-161e45c097b7'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sender_id_all[2579]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0093930f-35f6-4430-a6a8-420385a8245a\n",
      "0140324f-5374-46fb-9687-39f27f0da163\n",
      "01e32a17-40f7-46b1-a940-2a88a7e86865\n",
      "023490aa-6e64-4fb9-ac6b-97447397c58c\n",
      "02376ed0-5538-4aac-83e0-dbedae5c0166\n",
      "02990909-4ea9-4723-ab06-74242e0f245f\n",
      "03e792aa-b06c-49fc-919c-a4d552aa929f\n",
      "045cf039-9b8f-4a21-bb64-9c583ff39385\n",
      "068b004d-e5e4-48f1-83bd-4c4a6d083eb7\n",
      "069d24ff-8b8a-4218-b34b-23db972a7b24\n",
      "0704e170-1af0-4fd0-9139-8d61e00b8a43\n",
      "07e17870-c96a-413b-afa3-63d4a80f2c74\n",
      "092988b3-9050-4ed6-abea-d2500536f108\n",
      "092be56a-3023-47ce-b103-b284fcd54b56\n",
      "09c282ff-0e7c-49de-8ec7-d6b69736496d\n",
      "09e51eec-b8d7-4a93-b295-662afdb21541\n",
      "0a3cadb2-6bf9-4f5d-85ca-9bb7db8d2779\n",
      "0a5d1ac9-6e76-4ccf-a2e7-ad76cc4a1875\n",
      "0b589a3f-03ef-42d4-adbf-5211a9e8e42c\n",
      "0b987093-9478-4acb-bf90-759ea972ba5b\n",
      "0d20c19f-8c95-4e25-b48a-db85c4679767\n",
      "0dd0d9ca-6322-4d93-9c0f-2254a1e7560e\n",
      "0e3fe836-c4f4-43ce-8089-5b72dd28ebd7\n",
      "10722895-c2ab-4289-a18f-73e3fc3224d4\n",
      "10d0f56f-cd32-42c5-b932-c3afaa7c0ca1\n",
      "10df3030-7c91-46fe-a4c0-01e4543f99ea\n",
      "10f9a995-f801-4b72-a175-3e39a148c332\n",
      "114b4de1-a0e4-4a6d-a26e-e17c17774ba3\n",
      "11535203-6154-497c-b834-b3afa6a36697\n",
      "11f723de-9159-435f-a627-19d853d35409\n",
      "1213efcb-09bc-4d5a-81fa-5d9f22bf7dea\n",
      "159971b1-f930-4dc7-9c5b-f7df35cf41d6\n",
      "15de6be6-382f-42b7-959f-d1516c8b1e4f\n",
      "16d20e9f-3403-46ae-9380-1b33cae9ca2a\n",
      "16ddff9e-c68c-44d5-bba9-57d10cd81b90\n",
      "17364fa4-2e8d-4454-8b6c-9f5ec1b23df9\n",
      "17400c71-0121-4b40-b68f-71962a9e58db\n",
      "17da39c7-73d0-40dc-9292-570e4f5e3bb9\n",
      "183e9b4f-1dc6-4d2c-b5e8-bdd432b25311\n",
      "183eb7a4-aac5-4843-b49f-4d258f6ba2a4\n",
      "1858af4a-5ced-4d20-88a5-c364e1291182\n",
      "18bc89cc-ef17-4249-a18e-60f49930b9c2\n",
      "1a5755bb-7896-4fec-8e69-3bd6ec51f37b\n",
      "1aca66a9-9607-43fb-962d-de30f85449d7\n"
     ]
    }
   ],
   "source": [
    "for doc_id, sender_id in doc_sender_dict.items():\n",
    "    if sender_id == '0727e23c-ae0f-45e8-a110-161e45c097b7':\n",
    "        print(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "lexicon = pickle.load(open(DATA_FOLDER + 'lexicon.pickle', 'rb'))\n",
    "\n",
    "def is_valid_word(word):\n",
    "    import re\n",
    "    if len(word) < 1:\n",
    "        return False\n",
    "    if re.search('[a-zA-Z]', word) == None:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def parse_single_file(j):    \n",
    "    words = []\n",
    "    for word_info in j.get('describedWords'):\n",
    "        f = word_info.get('features')\n",
    "        try:\n",
    "            wordHight = word_info.get('bottom')\n",
    "            pageHeight = float(f.get('pageHeight'))\n",
    "            relativePosition = wordHight/pageHeight\n",
    "            \n",
    "            if relativePosition < 1.0/3 or relativePosition > 2.0/3:\n",
    "                word = word_info.get('text')\n",
    "                \n",
    "                if is_valid_word(word):\n",
    "                    local_words_list = tokenizer.tokenize(word.lower())\n",
    "                    words += local_words_list\n",
    "        except:\n",
    "            pass\n",
    "    return words\n",
    "\n",
    "def get_sender_name_via_id(doc_id):\n",
    "    import boto3\n",
    "    import botocore\n",
    "    import json\n",
    "    \n",
    "    b_name = 'ts-dev-cs-training-data'\n",
    "    prefix = 'cache_v2_individual/ubl1/hocr13/ubllight1/hocrlight1/sendertrainingmatrices20170819/data/'\n",
    "    key = prefix + doc_id + '.training'\n",
    "    \n",
    "    obj = boto3.resource('s3').Object(b_name, key)\n",
    "    doc_str = obj.get()['Body'].read().decode('utf-8')\n",
    "    doc_json = json.loads(doc_str)\n",
    "    \n",
    "    return doc_json['sender']\n",
    "\n",
    "def get_feature_via_words(words):\n",
    "    features = np.zeros(len(lexicon))\n",
    "    for word in words:\n",
    "        if word in lexicon:\n",
    "            index_value = lexicon.index(word)\n",
    "            features[index_value] += 1\n",
    "    return np.array([features])\n",
    "\n",
    "def get_words_via_id(doc_id):\n",
    "    import boto3\n",
    "    import botocore\n",
    "    import json\n",
    "    \n",
    "    b_name = 'ts-dev-cs-training-data'\n",
    "    prefix = 'rbp-research/data/e2e/2017-09-11/'\n",
    "    key = prefix + doc_id + '.training.json'\n",
    "    \n",
    "    obj = boto3.resource('s3').Object(b_name, key)\n",
    "    doc_str = obj.get()['Body'].read().decode('utf-8')\n",
    "    doc_json = json.loads(doc_str)\n",
    "    \n",
    "    words = parse_single_file(doc_json)\n",
    "    \n",
    "    words_in_lexicon = []\n",
    "    for word in words:\n",
    "        if word in lexicon:\n",
    "            words_in_lexicon.append(word)\n",
    "    return words_in_lexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tara Paletten-Handel- und Reparaturdienst GmbH\n",
      "['r', 'b', 'und', 'info', 'm', 'w', 'd', 'paletten', 't', 'g', 'p', 'u', 'd', 'w', 'w', 'h', 't', 'rechnung', 'zur', 'ta', 'r', 'm', 'd', 'e', 'e', 'e', 'r', 'd', 'w', 'd', 'em', 'd', 'bleibt', 'g', 'finance', 'w', 'w', 'g', 'unser', 'p', 'u', 'em', 'd', 'e', 'd', 'fax', 'rechnungsbetrag', 'ail', 'e', 'r', 'h', 'd', 'd', 'w', 'are', 'v', 'b', 'euro', 'b', 'w', 'ib', 'w', 'r', 'w', 'v', 'r', 'u', 't', 'h', 'n', 'r', 'u', 'bh', 'e', 'h', 'w', 'e', 'v', 'e', 'r', 'w', 'von', 'om', 'k', 'h', 'r', 'o', 'd', 'paletten', 'st', 'em', 'n', 'w', 'al', 'e', 'bis', 'tage', 'g', 'm', 'n', 'p', 'an', 'e', 'e', 'd', 'k', 'ie', 'g', 'ai', 'r', 'r', 'paletten', 'st', 'n', 'e', 'el', 'ib', 'd', 'fax', 'd', 'n', 'p', 'e', 'd', 'b', 'd', 'vollständigen', 'b', 'zahlbar', 'w', 'bh', 'w', 'tf', 'netto', 'c', 'u']\n",
      "2579\n"
     ]
    }
   ],
   "source": [
    "doc_id = '0093930f-35f6-4430-a6a8-420385a8245a'\n",
    "\n",
    "print(get_sender_name_via_id(doc_id))\n",
    "\n",
    "words = get_words_via_id(doc_id)\n",
    "#words = ['paletten', 'paletten']\n",
    "print(words)\n",
    "\n",
    "f = get_feature_via_words(words)\n",
    "\n",
    "f_tensor = torch.from_numpy(f).float()\n",
    "f_variable = Variable(f_tensor)\n",
    "\n",
    "out = net(f_variable)\n",
    "\n",
    "_, prediction = torch.max(out, 1)\n",
    "pred_y = prediction.data.numpy().squeeze()\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
