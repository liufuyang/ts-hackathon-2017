{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './sub_data_2/'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pickle.load(open(DATA_FOLDER + 'featureset.pickle', 'rb'))\n",
    "Y = pickle.load(open(DATA_FOLDER + 'labels_int.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np, X_dev_test_np, y_train_np, y_dev_test_np = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_dev_np, X_test_np, y_dev_np, y_test_np = train_test_split(X_dev_test_np, y_dev_test_np, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "normalizer = preprocessing.Normalizer().fit(X_train_np)\n",
    "\n",
    "#X_train_np = normalizer.transform(X_train_np)\n",
    "#X_dev_np = normalizer.transform(X_dev_np)\n",
    "#X_test_np = normalizer.transform(X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47773 10237 10238\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_np), len(X_dev_np), len(X_test_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47773, 2920)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2920 2699\n"
     ]
    }
   ],
   "source": [
    "_, feature_length = X_train_np.shape\n",
    "class_length =  len(np.unique(Y))\n",
    "\n",
    "print(feature_length, class_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train_np).float()\n",
    "y_train = torch.LongTensor(y_train_np)\n",
    "\n",
    "X_dev = torch.from_numpy(X_dev_np).float()\n",
    "y_dev = torch.LongTensor(y_dev_np)\n",
    "\n",
    "x, y = Variable(X_train), Variable(y_train)\n",
    "x_dev, y_dev = Variable(X_dev), y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential (\n",
      "  (0): Linear (2920 -> 200)\n",
      "  (1): ReLU ()\n",
      "  (2): Linear (200 -> 2699)\n",
      "  (3): LogSoftmax ()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(feature_length, 200),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(200, class_length),\n",
    "    torch.nn.LogSoftmax()\n",
    ")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.02)\n",
    "loss_func = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 -> Loss=7.912150, Train Acc=0.000, Dev Acc=0.278\n",
      "Epoch  2 -> Loss=5.771262, Train Acc=0.287, Dev Acc=0.418\n",
      "Epoch  3 -> Loss=4.177922, Train Acc=0.432, Dev Acc=0.583\n",
      "Epoch  7 -> Loss=1.454784, Train Acc=0.801, Dev Acc=0.810\n",
      "Epoch 12 -> Loss=0.488628, Train Acc=0.927, Dev Acc=0.900\n",
      "Epoch 17 -> Loss=0.196956, Train Acc=0.962, Dev Acc=0.929\n",
      "Epoch 22 -> Loss=0.103036, Train Acc=0.978, Dev Acc=0.940\n",
      "Epoch 27 -> Loss=0.063277, Train Acc=0.986, Dev Acc=0.942\n",
      "Epoch 32 -> Loss=0.042062, Train Acc=0.989, Dev Acc=0.947\n",
      "Epoch 37 -> Loss=0.030290, Train Acc=0.993, Dev Acc=0.948\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4c0209c9db68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# clear gradients for next train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tradeshift/ts-hackathon-2017/p3ml-venv/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Tradeshift/ts-hackathon-2017/p3ml-venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(40):\n",
    "    out = net(x)                 # input x and predict based on x\n",
    "    loss = loss_func(out, y)     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "\n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "    \n",
    "    if t % 5 == 1 or t in [0, 2]:\n",
    "        # plot and show learning process\n",
    "        _, prediction = torch.max(out, 1)\n",
    "        pred_y = prediction.data.numpy().squeeze()\n",
    "        target_y = y.data.numpy()\n",
    "        \n",
    "        \n",
    "        out_dev = net(x_dev)\n",
    "        _, prediction_dev = torch.max(out_dev, 1)\n",
    "        pred_y_dev = prediction_dev.data.numpy().squeeze()\n",
    "        target_y_dev = y_dev.numpy()\n",
    "\n",
    "        acc_train = sum(pred_y == target_y)/len(target_y)\n",
    "        acc_dev = sum(pred_y_dev == target_y_dev)/len(target_y_dev)\n",
    "        \n",
    "        print('Epoch{0:3d} -> Loss={1:.6f}, Train Acc={2:.3f}, Dev Acc={3:.3f}'.format(t+1, loss.data[0], acc_train, acc_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65835231, -0.40824829,  0.62347969],\n",
       "       [ 0.64544344,  0.81649658,  0.77934961],\n",
       "       [ 0.38726606,  0.40824829,  0.06234797]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X_train = np.array([[ 51, -1.,  2000.],\n",
    "                    [ 50.,  2.,  2500.],\n",
    "                   [ 30.,  1., 200.]])\n",
    "\n",
    "scaler = preprocessing.Normalizer().fit(X_train.T)\n",
    "scaler.transform(X_train.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Acc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.948427427232\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "print()\n",
    "\n",
    "X_test = torch.from_numpy(X_test_np).float()\n",
    "y_test = torch.LongTensor(y_test_np)\n",
    "\n",
    "x_test, y_test = Variable(X_test), y_test\n",
    "\n",
    "out_test = net(x_test)\n",
    "_, prediction_test = torch.max(out_test, 1)\n",
    "#pred_y_test = [randint(0, y_test.shape[0]) for x in range(y_test.shape[0])] \n",
    "pred_y_test = prediction_test.data.numpy().squeeze()\n",
    "target_y_test = y_test.numpy()\n",
    "\n",
    "acc_test = sum(pred_y_test == target_y_test)/len(target_y_test)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with some example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[394, 2685, 2579, 1584, 779]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_np[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sender_dict = pickle.load(open(DATA_FOLDER + 'doc_sender_dict.pickle', 'rb'))\n",
    "sender_id_all = pickle.load(open(DATA_FOLDER +'sender_id_all.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_id = sender_id_all[1584]\n",
    "print(sender_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Documents of sender {} are:'.format(sender_id))\n",
    "for doc_id, _sender_id in doc_sender_dict.items():\n",
    "    if _sender_id == sender_id:\n",
    "        print(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "lexicon = pickle.load(open(DATA_FOLDER + 'lexicon.pickle', 'rb'))\n",
    "\n",
    "def is_valid_word(word):\n",
    "    import re\n",
    "    if len(word) < 1:\n",
    "        return False\n",
    "    if re.search('[a-zA-Z]', word) == None:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def parse_single_file(j):    \n",
    "    words = []\n",
    "    for word_info in j.get('describedWords'):\n",
    "        f = word_info.get('features')\n",
    "        try:\n",
    "            wordHight = word_info.get('bottom')\n",
    "            pageHeight = float(f.get('pageHeight'))\n",
    "            relativePosition = wordHight/pageHeight\n",
    "            \n",
    "            if relativePosition < 1.0/3 or relativePosition > 2.0/3:\n",
    "                word = word_info.get('text')\n",
    "                \n",
    "                if is_valid_word(word):\n",
    "                    local_words_list = tokenizer.tokenize(word.lower())\n",
    "                    words += local_words_list\n",
    "        except:\n",
    "            pass\n",
    "    return words\n",
    "\n",
    "def get_sender_name_via_id(doc_id):\n",
    "    import boto3\n",
    "    import botocore\n",
    "    import json\n",
    "    \n",
    "    b_name = 'ts-dev-cs-training-data'\n",
    "    prefix = 'cache_v2_individual/ubl1/hocr13/ubllight1/hocrlight1/sendertrainingmatrices20170819/data/'\n",
    "    key = prefix + doc_id + '.training'\n",
    "    \n",
    "    obj = boto3.resource('s3').Object(b_name, key)\n",
    "    doc_str = obj.get()['Body'].read().decode('utf-8')\n",
    "    doc_json = json.loads(doc_str)\n",
    "    \n",
    "    return doc_json['sender']\n",
    "\n",
    "def get_feature_via_words(words):\n",
    "    features = np.zeros(len(lexicon))\n",
    "    for word in words:\n",
    "        if word in lexicon:\n",
    "            index_value = lexicon.index(word)\n",
    "            features[index_value] += 1\n",
    "    return np.array([features])\n",
    "\n",
    "def get_words_via_id(doc_id):\n",
    "    import boto3\n",
    "    import botocore\n",
    "    import json\n",
    "    \n",
    "    b_name = 'ts-dev-cs-training-data'\n",
    "    prefix = 'rbp-research/data/e2e/2017-09-11/'\n",
    "    key = prefix + doc_id + '.training.json'\n",
    "    \n",
    "    obj = boto3.resource('s3').Object(b_name, key)\n",
    "    doc_str = obj.get()['Body'].read().decode('utf-8')\n",
    "    doc_json = json.loads(doc_str)\n",
    "    \n",
    "    words = parse_single_file(doc_json)\n",
    "    \n",
    "    words_in_lexicon = []\n",
    "    for word in words:\n",
    "        if word in lexicon:\n",
    "            words_in_lexicon.append(word)\n",
    "    return words_in_lexicon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = '0da51d45-2069-406c-ae3c-631c62bef891'\n",
    "\n",
    "print(get_sender_name_via_id(doc_id))\n",
    "\n",
    "words = get_words_via_id(doc_id)\n",
    "#words = ['paletten', 'paletten', 'rechnungsbetrag']\n",
    "print(words)\n",
    "\n",
    "f = get_feature_via_words(words)\n",
    "\n",
    "f_tensor = torch.from_numpy(f).float()\n",
    "f_variable = Variable(f_tensor)\n",
    "\n",
    "out = net(f_variable)\n",
    "\n",
    "_, prediction = torch.max(out, 1)\n",
    "pred_y = prediction.data.numpy().squeeze()\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
